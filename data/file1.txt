A significant part of this can be thought of as a computational Python outlab - only difference, the requirements are actually open-ended and we don’t have autograders. Indeed, the initial description suggested an approach involving the “Bag of Words” strategy.

For each file, you could calculate its signature vector. For each file F[i], first compute a vector of integers that corresponds to the number of times each word occurs. You don’t need to store the actual word. Sort this vector. Call it the “signature”. The vectors probably won’t be of equal length, so you may have to pad them for what follows. For a fixed index j, normalise F[:, j], that is, subtract the mean and divide by standard deviation. 

As for the actual similarity, you’d have to interpret the right inner products appropriately (cosine similarity). If you’re using python, you have matplotlib, when it comes to visualising the data, you’re spoilt for choice. :)

If you’re enjoying CS 215 and have read up a bit on Machine Learning themes, you can incorporate that flavour in your solution. Instead of just the frequency, you can keep track of where in the document a particular word or phrase occurs. This is more of an “image” than a “signature”, you could try to see if the linear algebra you implement yields the patterns. 

If you’re feeling very ambitious, you could be even more hardcore and find the actual research papers about MOSS, and see how much of it is feasible for your team to implement yourselves, given that you have other courses, and of course, a life. 